#pragma once

#include "common/config.h"
#include "data/batch_stats.h"
#include "data/rng_engine.h"
#include "training/training_state.h"
#include "data/iterator_facade.h"
#include "3rd_party/threadpool.h"

#include <condition_variable>
#include <deque>
#include <functional>
#include <mutex>
#include <queue>

namespace marian {
namespace data {

// Iterator over batches generated by a BatchGenerator. Mean to be the only
// interface to create batches.
template <class BatchGenerator>
class BatchIterator : public IteratorFacade<BatchIterator<BatchGenerator>, 
                                            typename BatchGenerator::BatchPtr> {
private:
  BatchGenerator* bg_;
  typename BatchGenerator::BatchPtr current_;

  friend BatchGenerator;

  // private, only BatchGenerator is allowed to create pointers
  // hence friend class above.
  BatchIterator(BatchGenerator* bg, typename BatchGenerator::BatchPtr current) 
  : bg_(bg), current_(current) {}

public:
  
  virtual bool equal(const BatchIterator& other) const {
    // iterators are only equal if they point at the same batch or both have nullptr
    return current_ == other.current_;
  }

  // Just returns the batch pointer
  virtual const typename BatchGenerator::BatchPtr& dereference() const {
    return current_;
  }

  // sets current pointer to the next batch pointer, will be nullptr when no
  // batches are available. This will evaluate to false in a for loop.
  virtual void increment() {
    current_ = bg_->next();
  };
};

template <class DataSet>
class BatchGenerator : public RNGEngine {
public:
  typedef typename DataSet::batch_ptr BatchPtr;

  typedef typename DataSet::sample sample;
  typedef std::vector<sample> samples;     // @TODO: type names should be capitalized

  typedef BatchIterator<BatchGenerator> iterator;
  friend iterator;

protected:
  Ptr<DataSet> data_;
  Ptr<Config> options_;
  bool restored_{false};
  bool shuffle_;

private:
  Ptr<BatchStats> stats_;

  int batchSize_{1};

  typename DataSet::iterator current_;
  bool newlyPrepared_{true};

  size_t maxiBatchSize_;
  std::deque<BatchPtr> bufferedBatches_;
  BatchPtr currentBatch_;

  mutable ThreadPool threadPool_; // (we only use one thread, but keep it around)
  mutable std::mutex loadMutex_;
  mutable std::condition_variable loadCondition_;
  bool loadingSamples_{false};
  bool hadData_{false};

  // this runs on a bg thread; sequencing is handled by caller, but locking is done in here
  void fillBatches(bool shuffle = true) {
    //LOG(info, "fillBatches entered");
    typedef typename sample::value_type Item;
    auto itemCmp = [](const Item& sa, const Item& sb) { return sa.size() < sb.size(); }; // sort by element length, not content

    auto cmpSrc = [itemCmp](const sample& a, const sample& b) {
      return std::lexicographical_compare(
          a.begin(), a.end(), b.begin(), b.end(), itemCmp);
    };

    auto cmpTrg = [itemCmp](const sample& a, const sample& b) {
      return std::lexicographical_compare(
          a.rbegin(), a.rend(), b.rbegin(), b.rend(), itemCmp);
    };

    auto cmpNone = [](const sample& a, const sample& b) { return &a < &b; }; // instead sort by address, so we have something to work with

    typedef std::function<bool(const sample&, const sample&)> cmp_type;
    typedef std::priority_queue<sample, samples, cmp_type> sample_queue;

    std::unique_ptr<sample_queue> maxiBatch; // priority queue, shortest first

    if(options_->has("maxi-batch-sort")) {
      if(options_->get<std::string>("maxi-batch-sort") == "src")
        maxiBatch.reset(new sample_queue(cmpSrc));
      else if(options_->get<std::string>("maxi-batch-sort") == "none")
        maxiBatch.reset(new sample_queue(cmpNone));
      else
        maxiBatch.reset(new sample_queue(cmpTrg));
    } else {
      maxiBatch.reset(new sample_queue(cmpNone));
    }

    size_t maxBatchSize = options_->get<int>("mini-batch");
    size_t maxSize = maxBatchSize * options_->get<int>("maxi-batch");

    // LOG(info, "Preloading batches");

    // consume data from corpus into maxi-batch (single sentences)
    // sorted into specified order (due to queue)
    if(newlyPrepared_) {
      current_ = data_->begin();
      newlyPrepared_ = false;
    } else {
      if(current_ != data_->end())
        ++current_;
    }
    size_t sets = 0;
    while(current_ != data_->end() && maxiBatch->size() < maxSize) { // loop over data
      maxiBatch->push(*current_);
      sets = current_->size();
        // do not consume more than required for the maxi batch as this causes
        // that line-by-line translation is delayed by one sentence
        bool last = maxiBatch->size() == maxSize;
      if(!last)
        ++current_; // this actually reads the next line and pre-processes it
    }
    size_t numSentencesRead = maxiBatch->size();

    // LOG(info, "Turning samples into batches");

    // construct the actual batches and place them in the queue
    samples batchVector;
    size_t currentWords = 0;
    std::vector<size_t> lengths(sets, 0); // records maximum length observed within current batch

    std::vector<BatchPtr> tempBatches;
    tempBatches.reserve(10000); // (should be enough in most cases; not critical)

    // process all loaded sentences in order of increasing length
    // @TODO: we could just use a vector and do a sort() here; would make the cost more explicit
    //LOG(info, "begin form batches, #lines = {}", maxiBatch->size());
    const size_t mbWords = options_->get<size_t>("mini-batch-words", 0);
    const bool useDynamicBatching = options_->has("mini-batch-fit");
    BatchStats::const_iterator cachedStatsIter;
    if (stats_)
      cachedStatsIter = stats_->begin();
    while(!maxiBatch->empty()) { // while there are sentences in the queue
      // push item onto batch
      batchVector.push_back(maxiBatch->top());
      maxiBatch->pop(); // fetch next-shortest

      // have we reached sufficient amount of data to form a batch?
      bool makeBatch;
      if(useDynamicBatching && stats_) { // batch size based on dynamic batching
        for(size_t i = 0; i < sets; ++i)
          if(batchVector.back()[i].size() > lengths[i])
            lengths[i] = batchVector.back()[i].size(); // record max lengths so far

        maxBatchSize = stats_->findBatchSize(lengths, cachedStatsIter);
        // this optimization makes no difference indeed
#if 1     // sanity check: would we find the same entry if searching from the start?
        auto it = stats_->lower_bound(lengths);
        auto maxBatchSize1 = stats_->findBatchSize(lengths, it);
        ABORT_IF(maxBatchSize != maxBatchSize1, "findBatchSize iter caching logic is borked");
#endif
        makeBatch = batchVector.size() >= maxBatchSize;
        // if last added sentence caused a bump then we likely have bad padding, so rather move it into the next batch
        if(batchVector.size() > maxBatchSize) {
          maxiBatch->push(batchVector.back());
          batchVector.pop_back();
        }
      }
      else if(mbWords > 0) {
        currentWords += batchVector.back()[0].size(); // count words based on first stream =source  --@TODO: shouldn't we count based on labels?
        makeBatch = currentWords > mbWords; // Batch size based on sentences
      }
      else
        makeBatch = batchVector.size() == maxBatchSize; // Batch size based on words

      // if we reached the desired batch size then create a real batch
      if(makeBatch) {
        tempBatches.push_back(data_->toBatch(batchVector));

        // prepare for next batch
        batchVector.clear();
        currentWords = 0;
        lengths.assign(sets, 0);
        if (stats_)
          cachedStatsIter = stats_->begin();
      }
    }

    // turn rest into batch
    if(!batchVector.empty())
      tempBatches.push_back(data_->toBatch(batchVector));
    //LOG(info, "end form batches, #tempBatches = {}", tempBatches.size());

    // Shuffle the batches
    if(shuffle) {
      std::shuffle(tempBatches.begin(), tempBatches.end(), eng_);
    }
    //LOG(info, "end shuffling batches, #tempBatches = {}", tempBatches.size());

    // Wait here until batch buffer is empty;   
    // LOG(info, "Waiting for buffer to be empty");
    std::unique_lock<std::mutex> lock(loadMutex_);
    loadCondition_.wait(
        lock, [this] { return bufferedBatches_.empty(); });

    // put batches onto queue
    // exclusive lock
    // LOG(info, "Dumping batches to buffer");
    for(const auto& batch : tempBatches)
      bufferedBatches_.push_back(batch);
    LOG(debug, "[data] read {} sentences. Current batch queue size is {}", numSentencesRead, bufferedBatches_.size());

    loadingSamples_ = false;
    hadData_ = tempBatches.size() > 0;

    // Buffer is full now, everyone else can carry on
    loadCondition_.notify_all();
  }

  BatchPtr next() {
    // Start preloading batches and inform that loading is happening.
    // Detach the loading process so it's not blocking batch processing.
    {
      std::unique_lock<std::mutex> lock(loadMutex_);
      if(!loadingSamples_ && hadData_) {
try{
        loadingSamples_ = true;
#if 1
        threadPool_.enqueue([this]() {
        //std::thread([this]() {
          //pid_t gettid(void) { return syscall(SYS_gettid); }
          //LOG(info, "new thread for fillBatch with id {}", (pid_t)syscall(SYS_gettid));
          fillBatches(shuffle_); 
        });
        //.detach();
#else
        std::thread([this]() { 
          //pid_t gettid(void) { return syscall(SYS_gettid); }
          LOG(info, "new thread for fillBatch with id {}", (pid_t)syscall(SYS_gettid));
          fillBatches(shuffle_); 
        }).detach();
#endif
}
catch (const std::exception&) { // catch thread-handle leaks. @TODO: Remove this once no longer needed.
  LOG(info, "caught exception in Corpus::next()");
  system("ps -T -A");
  throw;
}
      }
    }
    
    // If there are no batches, but loading is happening,
    // wait for loading to finish. 
    {
      std::unique_lock<std::mutex> lock(loadMutex_);
      loadCondition_.wait(lock, [this] { 
        return !(loadingSamples_ && bufferedBatches_.empty()); 
      });
    }

    std::unique_lock<std::mutex> lock(loadMutex_);
    // Try to consume a batch
    if(bufferedBatches_.empty()) {
      // There was no batch in the buffer despite preloading -> end of epoch
      return nullptr;
    }
    else {
      auto batch = bufferedBatches_.front();
      bufferedBatches_.pop_front();

      // if(bufferedBatches_.size() % 100 == 0)
      //   LOG(info, "Buffered batches left {}", bufferedBatches_.size());

      // If there are no batches left, notify everyone who's waiting.
      // This will either dump buffered batches into the current queue,
      // or switch to the next epoch in the next call.
      if(bufferedBatches_.empty()) {
        // LOG(info, "Empty batches, notifying");
        loadCondition_.notify_all();
      }

      return batch;
    }
  }

public:

  BatchGenerator(Ptr<DataSet> data,
                 Ptr<Config> options,
                 Ptr<BatchStats> stats = nullptr)
      : data_(data), options_(options), stats_(stats), threadPool_(1) {}

  iterator begin() {
    return iterator(this, next());
  }

  iterator end() {
    return iterator(this, nullptr);
  }

  // @TODO: get rid of this function, begin() or constructor should figure this out
  void prepare(bool shuffle = true) {
    if(shuffle)
      data_->shuffle();
    else
      data_->reset();
    newlyPrepared_ = true;
    
    // @TODO: solve this better, maybe use options
    shuffle_ = shuffle;

    fillBatches(shuffle);
  }

  // Used to restore the state of a BatchGenerator after
  // an interrupted and resumed training.
  bool restore(Ptr<TrainingState> state, bool shuffle) {
    if(state->epochs == 1 && state->batchesEpoch == 0)
      return false;

    LOG(info,
        "[data] Restoring the corpus state to epoch {}, batch {}",
        state->epochs,
        state->batches);

    if(state->epochs > 1) {
      data_->restore(state);
      setRNGState(state->seedBatch);
    }

    prepare(shuffle);
    for(size_t i = 0; i < state->batchesEpoch; ++i)
      next();

    return true;
  }
};

class CorpusBatchGenerator : public BatchGenerator<CorpusBase>,
                             public TrainingObserver {
public:
  CorpusBatchGenerator(Ptr<CorpusBase> data,
                       Ptr<Config> options,
                       Ptr<BatchStats> stats = nullptr)
      : BatchGenerator(data, options, stats) {}

  void actAfterEpoch(TrainingState& state) override {
    state.seedBatch = getRNGState();
    state.seedCorpus = data_->getRNGState();
  }
};
}  // namespace data
}  // namespace marian
